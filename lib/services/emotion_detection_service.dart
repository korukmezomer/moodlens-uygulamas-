import 'dart:io';
import 'package:flutter/services.dart';
import 'package:tflite_flutter/tflite_flutter.dart';
import 'package:image/image.dart' as img;
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

class EmotionDetectionService {
  Interpreter? _interpreter;
  bool _isInitialized = false;
  
  // Emotion sÄ±nÄ±flarÄ± - Kendi eÄŸittiÄŸiniz modelin sÄ±ralamasÄ±
  // Model eÄŸitim scriptinizdeki sÄ±ralama:
  // emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
  // 
  // Flutter'da backend ile uyumlu olmasÄ± iÃ§in isimleri dÃ¼zeltiyoruz:
  // - 'disgust' -> 'disgusted'
  // - 'fear' -> 'fearful'  
  // - 'surprise' -> 'surprised'
  //
  // SÄ±ralama: 0: angry, 1: disgust, 2: fear, 3: happy, 4: neutral, 5: sad, 6: surprise
  final List<String> _emotions = [
    'angry',      // Index 0
    'disgusted',  // Index 1 (model'de 'disgust')
    'fearful',    // Index 2 (model'de 'fear')
    'happy',      // Index 3
    'neutral',    // Index 4
    'sad',        // Index 5
    'surprised'   // Index 6 (model'de 'surprise')
  ];

  /// TFLite modelini yÃ¼kle
  Future<void> initialize() async {
    if (_isInitialized) return;

    try {
      // Model dosyasÄ±nÄ± assets'ten yÃ¼kle
      // Not: Model dosyasÄ±nÄ± assets/models/ klasÃ¶rÃ¼ne eklemeniz gerekiyor
      // Ã–rnek: assets/models/emotion_model.tflite
      
      // Ã–nce model dosyasÄ±nÄ±n var olup olmadÄ±ÄŸÄ±nÄ± kontrol et
      try {
        final modelPath = 'assets/models/emotion_model.tflite';
        await rootBundle.load(modelPath);
        
        // Modeli yÃ¼kle
        _interpreter = await Interpreter.fromAsset(
          modelPath,
          options: InterpreterOptions()..threads = 4,
        );
        
        print('âœ… TFLite model yÃ¼klendi');
        _isInitialized = true;
      } catch (e) {
        print('âš ï¸ Model dosyasÄ± bulunamadÄ±: $e');
        print('âš ï¸ LÃ¼tfen emotion_model.tflite dosyasÄ±nÄ± assets/models/ klasÃ¶rÃ¼ne ekleyin');
        // Model yoksa servis Ã§alÄ±ÅŸmaya devam eder ama basit detection kullanÄ±r
        _isInitialized = false;
      }
    } catch (e) {
      print('âŒ TFLite model yÃ¼kleme hatasÄ±: $e');
      _isInitialized = false;
    }
  }

  /// YÃ¼z tespiti ve emotion detection yap
  Future<Map<String, dynamic>> detectEmotion(String imagePath) async {
    try {
      // 1. YÃ¼z tespiti yap
      final faceDetector = FaceDetector(
        options: FaceDetectorOptions(
          enableClassification: false,
          enableLandmarks: true,
          enableTracking: false,
        ),
      );

      final inputImage = InputImage.fromFilePath(imagePath);
      final faces = await faceDetector.processImage(inputImage);

      if (faces.isEmpty) {
        print('âš ï¸ YÃ¼z tespit edilemedi');
        return {
          'emotion': 'neutral',
          'confidence': 0.5,
          'error': 'No face detected'
        };
      }

      // Ä°lk yÃ¼zÃ¼ al
      final face = faces.first;
      print('âœ… YÃ¼z tespit edildi: ${face.boundingBox}');

      // 2. YÃ¼zÃ¼ kÄ±rp ve normalize et
      final imageFile = File(imagePath);
      final imageBytes = await imageFile.readAsBytes();
      final image = img.decodeImage(imageBytes);

      if (image == null) {
        throw Exception('Image decode failed');
      }

      // YÃ¼z bÃ¶lgesini kÄ±rp (padding ekleyerek daha iyi sonuÃ§ iÃ§in)
      final faceRect = face.boundingBox;
      final padding = 0.3; // %30 padding ekle (daha fazla context iÃ§in)
      final paddingX = (faceRect.width * padding).toInt();
      final paddingY = (faceRect.height * padding).toInt();
      
      final cropX = ((faceRect.left - paddingX).clamp(0, image.width - 1)).toInt();
      final cropY = ((faceRect.top - paddingY).clamp(0, image.height - 1)).toInt();
      final cropWidth = ((faceRect.width + paddingX * 2).clamp(1, image.width - cropX)).toInt();
      final cropHeight = ((faceRect.height + paddingY * 2).clamp(1, image.height - cropY)).toInt();
      
      var croppedImage = img.copyCrop(
        image,
        x: cropX,
        y: cropY,
        width: cropWidth,
        height: cropHeight,
      );
      
      // Contrast ve brightness iyileÅŸtirmesi (daha iyi detection iÃ§in)
      croppedImage = img.adjustColor(
        croppedImage,
        brightness: 1.1,  // %10 daha parlak
        contrast: 1.15,   // %15 daha kontrastlÄ±
        saturation: 0.0,  // Grayscale iÃ§in saturation yok
      );
      
      print('ğŸ“¸ YÃ¼z kÄ±rpÄ±ldÄ±: ${croppedImage.width}x${croppedImage.height}');

      // Model varsa TFLite ile detection yap
      if (_isInitialized && _interpreter != null) {
        return await _detectWithTFLite(croppedImage);
      } else {
        // Model yoksa basit rule-based detection
        return _detectWithRules(face);
      }
    } catch (e) {
      print('âŒ Emotion detection hatasÄ±: $e');
      return {
        'emotion': 'neutral',
        'confidence': 0.5,
        'error': e.toString()
      };
    }
  }

  /// TFLite model ile emotion detection
  Future<Map<String, dynamic>> _detectWithTFLite(img.Image image) async {
    try {
      // Model input boyutunu al (genellikle 48x48 veya 64x64)
      final inputShape = _interpreter!.getInputTensor(0).shape;
      final inputHeight = inputShape[1];
      final inputWidth = inputShape[2];
      
      // Model output shape'ini kontrol et
      final outputShape = _interpreter!.getOutputTensor(0).shape;
      print('ğŸ“Š Model Input Shape: $inputShape');
      print('ğŸ“Š Model Output Shape: $outputShape');

      // Resmi yeniden boyutlandÄ±r ve grayscale'e Ã§evir
      final resized = img.copyResize(
        image,
        width: inputWidth,
        height: inputHeight,
      );
      
      // Grayscale'e Ã§evir
      final grayscale = img.grayscale(resized);

      // Normalize et (0-1 arasÄ±)
      // BazÄ± modeller farklÄ± normalizasyon bekleyebilir, burada 0-1 arasÄ± kullanÄ±yoruz
      final inputList = <double>[];
      for (int y = 0; y < inputHeight; y++) {
        for (int x = 0; x < inputWidth; x++) {
          final pixel = grayscale.getPixel(x, y);
          // Grayscale iÃ§in red kanalÄ±nÄ± kullan (grayscale'de tÃ¼m kanallar aynÄ±)
          final value = pixel.r;
          // 0-1 arasÄ± normalize et (z-score normalization yerine min-max)
          final normalized = value / 255.0;
          inputList.add(normalized);
        }
      }
      
      print('ğŸ“Š Input normalization: min=${inputList.reduce((a, b) => a < b ? a : b).toStringAsFixed(3)}, max=${inputList.reduce((a, b) => a > b ? a : b).toStringAsFixed(3)}');
      
      // Reshape: [1, height, width, 1] formatÄ±na Ã§evir
      final input = List.generate(
        1,
        (_) => List.generate(
          inputHeight,
          (h) => List.generate(
            inputWidth,
            (w) => [inputList[h * inputWidth + w]],
          ),
        ),
      );

      // Model Ã§Ä±ktÄ±sÄ± iÃ§in buffer hazÄ±rla
      // Model output shape: [1, 7] (1 batch, 7 emotion classes)
      // Output shape'e gÃ¶re dinamik olarak oluÅŸtur
      final outputBatchSize = outputShape[0];
      final outputClassCount = outputShape.length > 1 ? outputShape[1] : _emotions.length;
      
      final output = List.generate(
        outputBatchSize,
        (_) => List.filled(outputClassCount, 0.0),
      );

      // Inference yap
      _interpreter!.run(input, output);

      // En yÃ¼ksek skorlu emotion'Ä± bul
      // output[0] = [1, 7] ÅŸeklindeki ilk batch, bu da 7 emotion score iÃ§erir
      final predictions = (output[0] as List).map((e) => e as double).toList();
      
      print('ğŸ“Š Output predictions: $predictions');
      print('ğŸ“Š Predictions count: ${predictions.length}, Emotions count: ${_emotions.length}');
      
      // GÃ¼venlik kontrolÃ¼: predictions sayÄ±sÄ± emotions sayÄ±sÄ±yla eÅŸleÅŸmeli
      if (predictions.length != _emotions.length) {
        throw Exception('Model output count (${predictions.length}) does not match emotions count (${_emotions.length})');
      }
      
      // TÃ¼m prediction skorlarÄ±nÄ± detaylÄ± logla
      print('ğŸ“Š DetaylÄ± prediction skorlarÄ±:');
      for (int i = 0; i < predictions.length; i++) {
        print('   Index $i (${_emotions[i]}): ${(predictions[i] * 100).toStringAsFixed(2)}%');
      }
      
      // En yÃ¼ksek 3 skoru gÃ¶ster (debug iÃ§in)
      final indexedPredictions = List.generate(
        predictions.length,
        (i) => {'index': i, 'emotion': _emotions[i], 'score': predictions[i]},
      );
      indexedPredictions.sort((a, b) => (b['score'] as double).compareTo(a['score'] as double));
      print('ğŸ“Š En yÃ¼ksek 3 skor:');
      for (int i = 0; i < 3 && i < indexedPredictions.length; i++) {
        final item = indexedPredictions[i];
        print('   ${i + 1}. Index ${item['index']} (${item['emotion']}): ${((item['score'] as double) * 100).toStringAsFixed(2)}%');
      }
      
      // En yÃ¼ksek skorlu emotion'Ä± bul
      double maxScore = 0.0;
      int maxIndex = 0;

      for (int i = 0; i < predictions.length; i++) {
        if (predictions[i] > maxScore) {
          maxScore = predictions[i];
          maxIndex = i;
        }
      }

      // Her zaman en yÃ¼ksek skorlu emotion'Ä± seÃ§
      final detectedEmotion = _emotions[maxIndex];
      final confidence = maxScore;

      print('ğŸ­ TFLite Detection: $detectedEmotion (${(confidence * 100).toStringAsFixed(1)}%) - Index: $maxIndex');
      
      // DÃ¼ÅŸÃ¼k gÃ¼ven uyarÄ±sÄ± (sadece bilgilendirme amaÃ§lÄ±, seÃ§imi etkilemez)
      if (confidence < 0.40) {
        print('âš ï¸ UYARI: DÃ¼ÅŸÃ¼k gÃ¼ven skoru tespit edildi (${(confidence * 100).toStringAsFixed(1)}%). Model yeniden eÄŸitilmeli veya daha kaliteli model kullanÄ±lmalÄ±.');
      }

      return {
        'emotion': detectedEmotion,
        'confidence': confidence,
        'isLowConfidence': confidence < 0.40, // Sadece bilgilendirme amaÃ§lÄ±
        'allPredictions': Map.fromIterables(
          _emotions,
          predictions.map((p) => p.toDouble()),
        ),
      };
    } catch (e) {
      print('âŒ TFLite inference hatasÄ±: $e');
      return {
        'emotion': 'neutral',
        'confidence': 0.5,
        'error': e.toString()
      };
    }
  }

  /// Basit rule-based detection (model yoksa)
  Map<String, dynamic> _detectWithRules(Face face) {
    // Basit kurallar: YÃ¼z Ã¶zelliklerine gÃ¶re tahmin
    // Bu sadece placeholder - gerÃ§ek detection iÃ§in model gerekli
    
    // Åimdilik rastgele bir emotion dÃ¶ndÃ¼r (test iÃ§in)
    // GerÃ§ek uygulamada model gerekli!
    
    print('âš ï¸ TFLite model yok, basit detection kullanÄ±lÄ±yor');
    
    return {
      'emotion': 'neutral',
      'confidence': 0.6,
      'note': 'Model not loaded - using fallback'
    };
  }

  /// Servisi temizle
  void dispose() {
    _interpreter?.close();
    _interpreter = null;
    _isInitialized = false;
  }
}


